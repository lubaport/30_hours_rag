# -*- coding: utf-8 -*-
"""1T.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QNP9rBGF6LTtyxsg4Yy1_LKbSTMJHiQG

## Install dependencies
"""

# Install necessary libraries
!pip install -qU langchain accelerate bitsandbytes transformers sentence-transformers faiss-gpu

# [ ] -q = "quiet", # U = "upgrade"
# [ ] langchain: A package that aims to make it easier to build applications with LLM.
#                Provides tools for document loading, embeddings, vector storage, and constructing pipelines
#                for operations like text generation, question answering, and more.

# [ ] accelerate: Hugging Face package. Designed to help with efficient training and inference of Transformers models on CPUs, GPUs, and TPUs.
#                 It simplifies running models on different hardware configurations.

# [ ] bitsandbytes: Offers optimized CUDA (Compute Unified Device Architecture) operations for training DNN.
#                   It's particularly useful for reducing memory usage and increasing the speed of certain operations in models, especially embedding layers.

# [ ] transformers: Hugging Face package. Provides thousands of pre-trained models for a wide range of NLP tasks.

# [ ] sentence-transformers: Library that makes it easy to use BERT-like models for sentence or text embeddings.
#                            These embeddings can be used for tasks such as semantic similarity, clustering, and more.

# [ ] faiss-gpu: FAISS (Facebook AI Similarity Search) is a library for efficient similarity search and clustering of dense vectors.
#                The faiss-gpu variant is optimized for use with NVIDIA GPUs, making it faster for tasks that involve searching through large sets of vectors.

"""## Import libraries"""

# Import required libraries and modules

import os  # Provides functions to interact with the operating system.
from glob import glob  # To find all the pathnames matching a specified pattern according to the rules used by the Unix shell.
import transformers  # Provides thousands of pre-trained models to perform tasks on texts such as classification, information extraction, question answering, summarization, translation, text generation, etc.

from transformers import (
                          AutoModelForCausalLM,  # Class, which is used to automatically retrieve a pre-trained causal language model from the Hugging Face hub.
                          AutoTokenizer,  # Automatically retrieve a tokenizer from the Hugging Face hub.
                          BitsAndBytesConfig,  # Configuration class for the BitsAndBytes integration in Transformers.
)


from langchain.document_loaders import TextLoader, PyPDFLoader  # For loading text and PDF documents.
from langchain.embeddings.huggingface import HuggingFaceEmbeddings  # Provides embeddings functionality using models from the Hugging Face hub.
from langchain.vectorstores import FAISS  # Helps efficiently search for nearest neighbors in large collections of vectors.
from langchain.chains import LLMChain  # Provides a way to create a chain of operations involving large language models.
from langchain.schema.runnable import RunnablePassthrough  # A utility class for running operations in a LangChain pipeline without modification.
from langchain.llms import HuggingFacePipeline  # An interface to run Hugging Face pipeline models.
from langchain.prompts import PromptTemplate  # For creating and managing prompt templates.
from langchain.text_splitter import CharacterTextSplitter  # Provides functionality to split text based on character count.
from langchain.chains import RetrievalQA  # Provides a chain for performing retrieval-based question answering.


#####################################################
import locale
#  [ ] locale: module that provides access to the POSIX locale database and functionality. It allows programmers to deal with various aspects of cultural conventions,
#              such as currency formats, date formats, and character encoding used by the system.

def getpreferredencoding(do_setlocale = True):
#   preferred encoding is UTF-8
    return "UTF-8"

locale.getpreferredencoding = getpreferredencoding

#####################################################
!pip3 install pypdf

from google.colab import drive
drive.mount('/content/drive')

# AutoComplete
from google.colab import auth
auth.authenticate_user()

""" ## Initialize Model and Tokenizer with BitsAndBytes Configuration"""

# pre_trained_model
model_id = "mistralai/Mistral-7B-Instruct-v0.1"
model_id_2 = "mistralai/Mistral-7B-Instruct-v0.2"

# Initialize tokenizer and set padding
tokenizer = AutoTokenizer.from_pretrained(model_id)  # Load a tokenizer for a pre-trained model from the Hugging Face model hub.
tokenizer.pad_token = tokenizer.eos_token  # Set the tokenizer's padding token to be the same as its end-of-sequence (EOS) token.
tokenizer.padding_side = "right"  # Post-padding for Transformer architecture



# Initialize the BitsAndBytesConfig (optimized CUDA operations for training DNN and efficient model loading)
# with specific quantization and computation settings.
bnb_config = BitsAndBytesConfig(load_in_4bit = True,  # Enable loading data in 4-bit precision - Makes model more feasible to run on devices with limited memory without significantly compromising performance.
                                bnb_4bit_quant_type = "nf4",  # Set the quantization type to "nf4" offering a balance between reducing memory usage and preserving model accuracy.
                                bnb_4bit_compute_dtype = "float16",  # Has High precision (more than torch.bfloat16)
                                bnb_4bit_use_double_quant = False,  # Disable double quantization to avoid extra complexity and potential accuracy loss.
)


# Load a pre-trained model (from Hugging Face) with additional configuration for quantization and sampling behavior.
model = AutoModelForCausalLM.from_pretrained(
      model_id,  # Specify the model identifier to load a specific pre-trained model
      quantization_config = bnb_config,  # Apply the previously defined bitsandbytes quantization configuration to optimize model size and inference speed.
      do_sample = True,  # Enable sampling mode for generating predictions, allowing for more varied and stochastic outputs.
)


tokenizer_2 = AutoTokenizer.from_pretrained(model_id_2)
tokenizer_2.pad_token = tokenizer_2.eos_token
tokenizer_2.padding_side = "right"
model_2 = AutoModelForCausalLM.from_pretrained(model_id_2, quantization_config = bnb_config, do_sample = True,)

print(f"Is tokenizer fast? = {tokenizer.is_fast}, {tokenizer_2.is_fast}")

"""## Set up Text Generation Pipeline"""

# Initialize a text generation pipeline using the transformers library, with custom settings for model, tokenizer, and generation parameters.
text_generation_pipeline = transformers.pipeline(
    model = model,  # Pass the pre-loaded model object for text generation.
    tokenizer = tokenizer,  # Use the pre-initialized tokenizer suitable for the model.
    temperature = 0.00001,  # Set a very low temperature for text generation, making output extremely deterministic.
    task = "text-generation",  # Specify the task as text generation.
    repetition_penalty = 1.1,  # Apply a slight penalty for repeating the same token, to encourage diversity.
    return_full_text = True,  # When True, returns the full text including both the prompt and generated text.
    max_new_tokens = 2000,  # Set a limit on the number of new tokens to generate (not counting the prompt).
)


# Create a HuggingFacePipeline instance for text generation
mistral_llm = HuggingFacePipeline(pipeline=text_generation_pipeline)


text_generation_pipeline_2 = transformers.pipeline(
    model = model_2,
    tokenizer = tokenizer_2,
    temperature = 0.00001,
    task = "text-generation",
    repetition_penalty = 1.1,
    return_full_text = True,
    max_new_tokens = 2000,
)
mistral_llm_2 = HuggingFacePipeline(pipeline=text_generation_pipeline_2)

"""## Prepare the Prompt Template that demands that the answer will be from the given context"""

# Define the prompt template for generating text
prompt_template = """
Instruction: prompt=Answer the following question:{question} based only on the provided context:{context}
If the answer is contained in the context, print "Answer:", and provide the answer from the context.
Also print "reference:" and show me from which part of the context your retrieved this answer.
If the answer does not appear in the context, answer: \"The answer isn't in the data you supplied\""

Question:
{question}
"""

prompt = PromptTemplate(
   input_variables=["context", "question"],
   template=prompt_template,
)


# prompt_template = """
# Instruction: Based only on the provided context, answer the following question.
# If the answer is contained in the context, print "Answer:" followed by the answer from the context.
# Also, print "reference:" and show from which part of the context you retrieved this answer.
# If the answer does not appear in the context, respond with "The answer isn't in the data you supplied."

# Question:
# {question}
# """

# prompt = PromptTemplate(
#    input_variables=["context", "question"],
#    template=prompt_template,
# )

"""## Create LLM Chain"""

# Create an instance of LLMChain, a component for chaining large language model (LLM) operations with a specific prompt and LLM.
llm_chain = LLMChain(llm = mistral_llm,  # Pass the LLM (in this case, 'mistral_llm') to be used in the chain.
                     prompt = prompt  # Specify the prompt that will be used as input for the language model.
)


llm_chain_2 = LLMChain(llm = mistral_llm_2, prompt = prompt)

"""##  Load and Process PDF Documents"""

import copy
from langchain_community.document_loaders import PyPDFLoader

# path = '../gdrive/My Drive/Colab Notebooks/RAG/Harry Potter and the Sorcerers Stone.pdf'
path = "Harry Potter and the Sorcerers Stone.pdf"
loader = PyPDFLoader(path)
pages = loader.load_and_split()

# Create a deep copy of the pages list to retain original objects intact
cleaned_pages = copy.deepcopy(pages)

# Clean the page_content of each page in the cleaned_pages list
for page in cleaned_pages:
    page.page_content = page.page_content.replace('\t', ' ')  # Replace tab characters with spaces



## Index Documents with FAISS
###############################
# Create a FAISS index from the cleaned documents using sentence embeddings for efficient similarity search.
embeddings_model = HuggingFaceEmbeddings(model_name='sentence-transformers/all-mpnet-base-v2')

db = FAISS.from_documents(
    cleaned_pages,  # a list of strings, each representing a cleaned document/page.
    embeddings_model # converting text to vector space
)

# Convert the FAISS vector index into a retriever capable of handling search queries.
retriever = db.as_retriever()  # This creates a retriever object from the FAISS index for querying similar documents.

"""## Save the retriever db to the disk (or cloud)"""

import pickle
import shutil

# serializes the obj object (retriever) into a binary format and writes it to a file named filename.
# pickle.HIGHEST_PROTOCOL specifies using the highest available protocol version for the pickle format,
# Serialization is the process of converting an object's state into a format that can be stored or transmitted and subsequently reconstructed.
def save_object(obj, filename):
    with open(filename, 'wb') as outp:  # Overwrites any existing file.  [w = write mode, b = Binary]
    # w = allows to write data to the file. [If file exists = overwritten, If not = creates]
        pickle.dump(obj, outp, pickle.HIGHEST_PROTOCOL)

# Reads binary file specified by filename, deserializes it, and returns the original Python object.
# Deserialization is the reverse of serialization - it reconstructs the object from its serialized state.
def load_object(filename):
    with open(filename, 'rb') as inp:  # Open the file in binary read mode
        obj = pickle.load(inp)
    return obj



save_object(retriever, 'hp_retriever.pkl')
source_path = '/content/hp_retriever.pkl' # Source path of the file (the one we want to copy)
destination_path = '/content/drive/My Drive/Colab Notebooks/retriever_file_hp'  # Update this path

# Copying the file
shutil.copy(source_path, destination_path)
print(f"File copied to Google Drive successfully: {destination_path}")

loaded_retriever= load_object(destination_path)

"""## Ask you question about the data + Concat the relevant content to create a string context"""

# question = "who is obama?"
# question = "who is Harry?"
# question = "who is Harry Potter?"
# question = "who is Hermione?"
question = "who is better - Dumbledore or Snape. And why?"

# Takes a query (=question) as its input and returns a list or collection of documents that are relevant to the query.
# It searches a database, executes a semantic search over a corpus of documents
docs = loaded_retriever.get_relevant_documents(question)
context = " ".join(doc.page_content for doc in docs)
result = llm_chain.run(context=context, question=question)
print(result)

result_2 = llm_chain_2.run(context=context, question=question)
print(result_2)

# import torch
# from transformers import AutoModelForCausalLM, AutoTokenizer

# class Chatbot:
#     def __init__(self):
#         self.tokenizer = AutoTokenizer.from_pretrained("microsoft/DialoGPT-medium")
#         self.model = AutoModelForCausalLM.from_pretrained("microsoft/DialoGPT-medium")
#         self.chat_history_ids = None

#     def reset_context(self):
#         self.chat_history_ids = None

#     # Modify the get_response method to include do_sample=True in the model.generate call

#     def get_response(self, message):
#         # Encode the new user message
#         new_user_input_ids = self.tokenizer.encode(message + self.tokenizer.eos_token, return_tensors='pt')

#         # Append the new user input tokens to chat history, if it exists
#         bot_input_ids = torch.cat([self.chat_history_ids, new_user_input_ids], dim=-1) if self.chat_history_ids is not None else new_user_input_ids

#         # Generate a response with do_sample=True to enable sample-based generation
#         self.chat_history_ids = self.model.generate(
#             bot_input_ids,
#             max_length=1000,
#             pad_token_id=self.tokenizer.eos_token_id,
#             temperature=0.7,
#             do_sample=True
#         )

#         # Extract the response
#         response = self.tokenizer.decode(self.chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)

#         return response

# chatbot = Chatbot()

# # Example conversation
# print("Bot: Hi! How can I help you today?")
# while True:
#     message = input("You: ")
#     if message.lower() == "quit":
#         break
#     response = chatbot.get_response(message)
#     print(f'You: {message}')
#     print(f"Bot: {response}")



"""## Execute chain and show response"""